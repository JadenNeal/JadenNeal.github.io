<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    强化学习简介 |  Jaden Neal&#39;s Blog
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<meta name="generator" content="Hexo 4.2.0"></head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-294-3" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  强化学习简介
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2019/12/31/294-3/" class="article-date">
  <time datetime="2019-12-31T11:02:43.000Z" itemprop="datePublished">2019-12-31</time>
</a>
      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>本节内容：</p>
<ol>
<li>MDP（马尔可夫决策过程）的定义</li>
<li>强化学习问题的定义</li>
<li>详解RL算法</li>
<li>RL算法类型概述</li>
</ol>
<a id="more"></a>

<h2 id="MDP（马尔科夫决策过程）"><a href="#MDP（马尔科夫决策过程）" class="headerlink" title="MDP（马尔科夫决策过程）"></a>MDP（马尔科夫决策过程）</h2><h3 id="重要性"><a href="#重要性" class="headerlink" title="重要性"></a>重要性</h3><p>可以毫不夸张地说，MDP定义了RL的基本世界观。它有state, action, reward, transition，这些东西恰好就是RL世界的基础。</p>
<h3 id="马尔可夫链"><a href="#马尔可夫链" class="headerlink" title="马尔可夫链"></a>马尔可夫链</h3><p>马尔可夫链是由状态空间和转移概率组成的。即：$$M = \{S, T \}$$<br>其中\(S\)表示<strong>状态</strong>，\(T\)表示<strong>转移算子</strong>，<strong>下一状态只与当前状态有关。</strong></p>
<h3 id="完全可观察的MDP"><a href="#完全可观察的MDP" class="headerlink" title="完全可观察的MDP"></a>完全可观察的MDP</h3><p>$$M = \{S, A, T, r\}$$<br>其中：</p>
<ul>
<li>S：状态空间。离散或者连续的均可</li>
<li>A：动作空间。离散或者连续的均可</li>
<li>T：转移算子，是一个<strong>张量</strong>！</li>
<li>r：激励函数，\(R ← S \times A\)</li>
</ul>
<center><img src="https://s2.ax1x.com/2019/12/31/l3lKqH.jpg" alt="l3lKqH.jpg" border="0" />

<p>这里S和A一起决定了下一状态</center></p>
<h3 id="部分可观察的MDP——POMDP"><a href="#部分可观察的MDP——POMDP" class="headerlink" title="部分可观察的MDP——POMDP"></a>部分可观察的MDP——POMDP</h3><p>这是MDP最一般的形式，但是，我们并不经常遇到这种形式。此时，则有：$$M = \{S, A, O, T, \epsilon, r\}$$<br>其中：</p>
<ul>
<li>S：状态空间</li>
<li>A：动作空间</li>
<li>O：观察空间</li>
<li>T：转移算子</li>
<li>\(\epsilon\)：输出概率</li>
<li>r：激励函数，也可以称作是奖励函数</li>
</ul>
<h2 id="强化学习的目标"><a href="#强化学习的目标" class="headerlink" title="强化学习的目标"></a>强化学习的目标</h2><p>找到一组参数，能够使得所有动作得到奖励总和的期望值最大。<br>$$p_{\theta}(s_1, a_1, \dots, s_T, a_T) = p(s_1)\prod_{t=1}^T\pi_{\theta}(a_t|s_t)p(s_{t+1}|s_t, a_t)$$</p>
<p>$$\theta^* = \arg max(\theta)E_{\tau \sim p_{\theta}(\tau)}\left[\sum_tr(s_t, a_t)\right]$$<br>这里之所以要写成期望的形式，是因为在给定参数向量下， 状态和行动仍然是随机变量，因此要求的是期望。</p>
<h2 id="强化学习算法"><a href="#强化学习算法" class="headerlink" title="强化学习算法"></a>强化学习算法</h2><p>分成三部分：</p>
<ul>
<li>产生样本</li>
<li>拟合模型/估计回报（最为耗时）</li>
<li>改进策略</li>
</ul>
<p>如何应对随机系统呢？答案是使用条件概率。</p>
<h2 id="Q函数和V函数"><a href="#Q函数和V函数" class="headerlink" title="Q函数和V函数"></a>Q函数和V函数</h2><h3 id="Q函数定义"><a href="#Q函数定义" class="headerlink" title="Q函数定义"></a>Q函数定义</h3><p>Q函数的定义是状态-动作值函数，是对每一个时间步进行定义。它是一个对特定策略\(\pi\)的、表示累计奖励的函数。<br>$$Q^{\pi}(s_t, a_t) = \sum_{t’=t}^{T}E_{\pi\theta}[r(s_{t’}, a_{t’})|s_t, a_t]$$</p>
<h3 id="V函数定义"><a href="#V函数定义" class="headerlink" title="V函数定义"></a>V函数定义</h3><p>V函数是指值函数，也是表示累计奖励的函数。有以下两种定义方法。</p>
<ol>
<li>\(V^{\pi}(s_t) = \sum_{t’=t}^{T}E_{\pi\theta}[r(s_{t’}, a_{t’})|s_t]\)</li>
<li>\(V^{\pi}(s_t) = E_{a_t\sim\pi(a_t|s_t)}[Q^{\pi}(s_t, a_t)]\)</li>
</ol>
<p><strong>上述两种定义是等价的。</strong></p>
<h2 id="算法种类简介"><a href="#算法种类简介" class="headerlink" title="算法种类简介"></a>算法种类简介</h2><p>$$\theta^{*} = \arg max(\theta)E_{\tau\sim p_{\theta}(\tau)}\left[\sum_tr(s_t, a_t)\right]$$</p>
<ol>
<li>策略梯度算法（Policy Gradients）：目的是计算上述\(\theta^*\)的导数来计算期望。</li>
<li>基于值的算法：直接估计优化策略的Q函数或V函数，并通过\(\arg max\)来得到最优解。</li>
<li>Actor-critic算法：这是上述二者的结合。估计现有策略的V函数和Q函数，利用它来改进策略。</li>
<li>基于模型的强化学习：估计传递模型，然后可能采取不同的做法，比如：<ul>
<li>使用它（指估计出来的模型，下同）来做出计划（不需要显式的策略）</li>
<li>使用它来改进策略</li>
<li>其他</li>
</ul>
</li>
</ol>
<h2 id="为什么有许多RL算法？"><a href="#为什么有许多RL算法？" class="headerlink" title="为什么有许多RL算法？"></a>为什么有许多RL算法？</h2><ol>
<li><p>不同的折中</p>
<ul>
<li>不同的样本效率（sample efficiency）。比如对同一算法来说，用100万个样本能得到1000个反馈值，而另一个算法用1000个样本也能达到同一效果，那么就可以说第二种算法更有样本效率。</li>
<li>稳定性和易用性。这两种性质并没有精确的定义。但我们可以用一种方法进行“测量”。运行N次算法，有多大频率能收敛到一个较好的值，需要怎样精心选择超参数来取得好的收敛效果。</li>
</ul>
</li>
<li><p>不同的假设</p>
<ul>
<li>随机的或者是确定的。这里可以指动态系统，也可以指策略。</li>
<li>连续的或离散的。</li>
<li>有限期的或无限期的。</li>
</ul>
</li>
<li><p>不同设置的难易程度<br> 这里说的是对于特定的问题，表示策略更为容易，还是表示模型更容易。  </p>
<ul>
<li>表示策略更容易</li>
<li>表示模型更容易</li>
</ul>
</li>
</ol>
<h2 id="样本效率"><a href="#样本效率" class="headerlink" title="样本效率"></a>样本效率</h2><p><strong>样本效率</strong>：需要多少样本来得到一个好的策略？<br>有很多因素影响着算法是否有效，但<strong>是否是离线的算法</strong>是最重要的一个。</p>
<p><strong>离线策略：</strong>能够用样本来改进策略，而不需要从该策略中生成新的样本。<br><strong>在线策略：</strong>每当策略改变时，哪怕只是一点点，我们都需要生成新的样本。</p>
<div align="center"><img src="https://s2.ax1x.com/2020/01/05/lBHU8H.jpg" alt="lBHU8H.jpg" border="0" /></div>
<center>各种算法样本效率一览</center>

<h3 id="为什么要使用低效率算法"><a href="#为什么要使用低效率算法" class="headerlink" title="为什么要使用低效率算法"></a>为什么要使用低效率算法</h3><p>继续讨论的一个问题是，根据上面的图片，有些算法样本效率很低，为什么我们还会继续使用呢？<br>答案是<strong>高效率并不意味着该算法一定好。</strong>样本效率并不是算法优劣的唯一度量。</p>
<p>算法真正的执行时间，也就是需要等待的时间。当使用一个很简单但可以并行的模拟器，可能样本效率不高，但却缩短了执行时间。这也就是人们仍然在使用进化策略之类的算法的原因。</p>
<h2 id="稳定性和易用性"><a href="#稳定性和易用性" class="headerlink" title="稳定性和易用性"></a>稳定性和易用性</h2><p>有几个重要的问题： </p>
<ol>
<li>稳定性和易用性可以使算法收敛吗？</li>
<li>如果收敛，收敛到什么结果呢？</li>
<li>每次运行都收敛吗？</li>
</ol>
<p>上面的问题看似来似乎有点莫名其妙，因为在监督学习，或者在非监督学习情况下使用比梯度下降更加规范的算法（比如凸优化）时，我们的目标就是使用梯度下降来得到局部最优。因此上述的问题总是有确切的答案：<strong>会收敛、每次都收敛、收敛到局部最优。</strong></p>
<p>但是在强化学习领域就有必要来讨论这三个问题了，因为许多RL算法都不是梯度下降的。 这些算法可能需要看起来像梯度下降的式子，但事实上却不是在做梯度下降。比如：</p>
<ol>
<li>Q学习：所求的Q函数并不是在做梯度下降，而是在做固定点循环，来收敛到非常简单的函数类（一般不收敛到复杂的近似函数，比如NN收敛的结果就是复杂的近似函数）。</li>
<li>基于模型的RL：该算法是通过梯度下降去优化模型，但优化模型并不是像优化反馈函数那样，人们通过拟合数据得到最好的模型去预测。<strong>这确实是梯度下降，也是监督学习，但并不是显式地最大化价值函数，而是设法给出一个更精确的模型。</strong>这会带来更好的策略。但这一过程并不确定，因为一些模型错误，可能使得得到反馈函数的代价更加高昂。</li>
<li>策略梯度：它就是梯度下降，但同时也是样本效率最低的。因此当它收敛且收敛到局部最优值时，它需要的样本最多。</li>
</ol>
<p><strong>现在正式讨论上述的三个问题</strong></p>
<ol>
<li>价值函数拟合：最好的结果：拟合价值函数时，人们最小化的是拟合的错误（也称Bellman error）。也就是输出的价值和估计的价值之间的差距。但是，<strong>缩小二者的差距，并不能保证产生一个好的策略。但如果错误率能降为0，就可以得到一个好的策略。</strong><br>最差的结果，就是并没有优化任何结果。就连许多流行的深度RL价值函数拟合算法都不能保证一定收敛，尤其是在非线性场景中。</li>
<li>基于模型的RL：模型会最小化拟合结果，这可以保证收敛。但是，却不能保证会得到一个更好的策略。</li>
<li>策略梯度：这是唯一一个在实际物体上应用梯度下降（或上升）的算法，这会使得策略梯度算法易用性提高，但也会带来许多问题，比如高方差。</li>
</ol>
<h2 id="不同的假设"><a href="#不同的假设" class="headerlink" title="不同的假设"></a>不同的假设</h2><p>有三大假设，这里只列举最普遍的。</p>
<ol>
<li><p>假设1：系统是可以全程观察的。这意味着我们可以根据观测或者状态去训练策略。有些假设是不可观察的，那就是符合马尔可夫属性。</p>
<ul>
<li>一般来说，这种带有欺骗性观测是通过价值函数拟合来假设的。这种方法直接拟合价值函数或者Q函数，隐含假设是有一个\(Markov\)状态 。</li>
<li>因此可以通过循环的值函数或者Q函数去迁移这种状态</li>
</ul>
</li>
<li><p>假设2：情境学习（\(episodic\quad learning\)）<br>这种情况下学习过程被组织成一个个情景序列。</p>
<ul>
<li>情境学习是通过直接策略梯度方法来假设。</li>
<li>这种算法是很多基于模型的算法的假设。</li>
</ul>
</li>
<li><p>假设3：连续性和光滑性</p>
<ul>
<li>一些连续价值函数学习方法通常是这样假设的；</li>
<li>一些基于模型的RL算法也是如此假设。</li>
</ul>
</li>
</ol>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://jadenneal.github.io/2019/12/31/294-3/" data-id="ck4trkp0p0000rcpb6sul70l2"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
    
      <a href="/2019/12/30/294-2/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">第二讲-监督学习和模仿学习（二）</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: '',
        app_key: '',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '想对作者说点什么',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2016-2020
        Jaden Neal
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Jaden Neal&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">目录</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E9%80%9A%E4%BF%A1%E7%A7%91%E6%99%AE">通信科普</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/latex">latex</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%85%B6%E4%BB%96">其他</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  
  </div>
</body>

</html>