<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    优先经验回放 |  Jaden Neal&#39;s Blog
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<meta name="generator" content="Hexo 4.2.0"></head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-prioritized-experience-replay" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  优先经验回放
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/prioritized-experience-replay/" class="article-date">
  <time datetime="2020-03-19T14:07:04.865Z" itemprop="datePublished">2020-03-19</time>
</a>
      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p><strong>之前工作的问题</strong>：经验过渡是从回放记忆中均匀采样，但这种方法仅仅以同一频率回放过渡，而不管这些经验的重要性。<br><strong>本文创新点</strong>：为<strong>优先经验</strong>提出一种框架，对于重要的过渡，回放就较多，因此学习起来更有效率。<br><strong>实验过程与结果</strong>：<strong>将优先经验回放应用到DQN上</strong>，获得了超越传统DQN的结果。49个游戏中有41个都超越了，足以说明本文提出的新框架的优越性。</p>
<a id="more"></a>

<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在线RL有两个问题：</p>
<ol>
<li>相邻更新极度相关，基于梯度的随机算法不再适用。</li>
<li>一些可能有用的罕见经验会被快速丢弃。</li>
</ol>
<p>因此提出了经验回放（<code>Experience Replay</code>），将经验存储在回放记忆中，有利于打断<strong>相关性</strong>，从而减少了需要学习的经验，用更多的计算和记忆来代替。</p>
<p>本论文就是研究了<strong>优先性</strong>对<strong>经验回放</strong>所造成的影响，用TD误差来衡量优劣。由于优先性会导致多种误差，因此该论文又引入了随机优先性，加入偏差，用重要性采样来修正。</p>
<h2 id="算法设计"><a href="#算法设计" class="headerlink" title="算法设计"></a>算法设计</h2><p>经验回放设计有两种选择：1、存储哪种经验；<strong>2、回放哪种经验</strong>。本论文解决的是后者。</p>
<h3 id="TD误差的优先级"><a href="#TD误差的优先级" class="headerlink" title="TD误差的优先级"></a>TD误差的优先级</h3><p><strong>经验回放</strong>的核心部分是每次过渡的重要性的衡量标准，一个理想的标准就是RL智能体从当前状态的过渡中学习到的知识数量。然而这种方法不能直接使用，<strong>必须用TD误差来代替</strong>。<br>优点：对于那种递增式的、在线的RL算法非常有效，比如<code>SARSA</code>，<code>Q-learning</code><br>缺点：对于噪声很大的奖励则效果很差。</p>
<h3 id="随机优先性"><a href="#随机优先性" class="headerlink" title="随机优先性"></a>随机优先性</h3><p>TD误差优先有一系列的问题：</p>
<ol>
<li>为了避免对整个记忆扫描，TD误差只会更新回放过的过渡，造成的结果就是初次有着低TD误差的过渡就不会被重放太长时间。</li>
<li>对噪声敏感（比如当奖励是随机的时候），并且会随着<code>bootstraping</code>恶化，其中逼近误差就像来自另外一个噪声源。</li>
<li>它只对经验的一小个子集操作——误差缓慢下降（特别是做函数逼近的时候），意味着初始的高误差过渡重放频繁，这就造成了多样性缺失，即同一性太强，容易造成模型过拟合。</li>
</ol>
<p>为了克服这些问题，论文中引入了一种随机抽样方法，插入在纯贪婪排序与均匀随机抽样中。作者还确保了采样可能性是单调的。</p>
<h3 id="偏差退火"><a href="#偏差退火" class="headerlink" title="偏差退火"></a>偏差退火</h3><p>优先回放引入了偏差，由于其改变了分布，从而也改变了估计收敛的方法（即使策略和状态分布都是固定的）。论文用下式更正了此偏差：<br>$$w_i = \left(\frac{1}{N}·\frac{1}{P(i)}\right)^\beta$$ 其中$\beta$称为<strong>重要性权重（IS）</strong>。<br>若$\beta=1$，就能补偿非均匀概率$P(i)$，当然为了稳定性，也可以用$1/\max_iw_i$来正则化。</p>
<p>论文假设小的偏差可以忽略，于是通过改变$\beta$，来探索重要性采样更正的退火数量的灵活性。<br>重要性采样（IS）的优点不止于此，由于一阶梯度逼近只是局部可靠，对于大的步长则是毁灭性的结果，因此必须用较小的步长来避免这种结果。论文结果证明，<strong>优先性</strong>确保高误差的过渡会使用多次，尽管IS纠正减少了梯度空间，使得算法可以沿着非线性优化最快的方向进行。</p>
<h2 id="Atari实验"><a href="#Atari实验" class="headerlink" title="Atari实验"></a>Atari实验</h2><h3 id="实验简介"><a href="#实验简介" class="headerlink" title="实验简介"></a>实验简介</h3><p>环境：Atari游戏<br>游戏挑战：延迟积分分配、部分可观察的环境、较为困难的函数逼近<br>baseline：DQN、Double DQN，均使用均匀经验回放</p>
<h3 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h3><ol>
<li>对于Baseline，使用理想的神经网络架构、学习算法、回放记忆和估计设置。<strong>唯一的不同是采样机制</strong>，使用的是改进的算法而不是均匀采样。</li>
<li>baseli和论文改进的比较只有一个超参数：假定优先回放会倾向于高误差过渡，导致梯度空间也会非常大，所以论文把步长大小$\eta$减少到了<code>Double DQN</code>的4倍</li>
<li>将baselines与上面提到的两种优先回放进行比较。</li>
<li>对于超参数$\alpha$和$\beta_0$，做了一个比较粗糙地计算，得到两种情况下的最优值。<ul>
<li>基于排序的变量：$\alpha = 0.7, \beta_0 = 0.5$</li>
<li>对于比例变量：$\alpha = 0.6, \beta_0 = 0.4$</li>
</ul>
</li>
<li>在所有游戏中使用一个超参数设置运行每个变量来生成结果，baseli也这么做。<ul>
<li>评估指标：最好策略的质量</li>
</ul>
</li>
<li>结果比较</li>
</ol>
<table>
<thead>
<tr>
<th></th>
<th>DQN</th>
<th></th>
<th>Double DQN（修正后）</th>
<th></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td></td>
<td>baseline</td>
<td>rank-based</td>
<td>baseline</td>
<td>rank-based</td>
<td>proportional</td>
</tr>
<tr>
<td>Median</td>
<td>48%</td>
<td>106%</td>
<td>111%</td>
<td>113%</td>
<td>128%</td>
</tr>
<tr>
<td>Mean</td>
<td>122%</td>
<td>355%</td>
<td>418%</td>
<td>454%</td>
<td>551%</td>
</tr>
<tr>
<td>&gt;baseline</td>
<td>-</td>
<td>41</td>
<td>-</td>
<td>38</td>
<td>42</td>
</tr>
<tr>
<td>&gt;human</td>
<td>15</td>
<td>25</td>
<td>30</td>
<td>33</td>
<td>33</td>
</tr>
<tr>
<td>#games</td>
<td>49</td>
<td>49</td>
<td>57</td>
<td>57</td>
<td>57</td>
</tr>
<tr>
<td>可以发现效果有了很大改善。</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>对于<strong>基于排序的优先性和比例优先性</strong>，有下面两种可能。</p>
<ol>
<li>基于排序的优先性更好：<ul>
<li>优点：<ul>
<li>不会被异常影响，因而鲁棒性更强</li>
<li>满足<strong>重（zhong）尾</strong>分布，保证了样本的多样性。</li>
<li>从不同的误差中分层采样，使得在训练过程中保证了总的小批次的梯度在一个稳定的大小。</li>
</ul>
</li>
<li>缺点：rank使得算法忽视了相关错误的尺度，以致于会在利用错误分布时模型的表现不好。</li>
</ul>
</li>
<li>也可能二者差不多。那么此时可能是因为<strong>在DQN算法中过多地使用了“剪枝”，从而去掉了异常</strong>。</li>
</ol>
<p>除此之外，论文还发现了一个奇怪的现象：<strong>访问过的过渡的一些片段，在被从滑动窗口记忆丢弃之前，它们从不会被回放</strong>，就算回放了，更多的还是第一次。</p>
<p>均匀采样暗含误差，因此优先回放解决了该问题，也会帮助解决第二大问题——时间上越近的过渡误差越大。这是由于旧的过渡有更多机会纠正自身错误，也是因为新的数据不太容易被值函数预测到。</p>
<h2 id="拓展工作"><a href="#拓展工作" class="headerlink" title="拓展工作"></a>拓展工作</h2><ol>
<li>优先监督学习：从数据集非均匀采样，每个样本都应用基于上次“看到”的优先性</li>
<li>离线策略回放：对于离线RL策略来说，有两种标准方法：剔除采样和重要性采样。</li>
<li>探索反馈</li>
<li>优先记忆</li>
</ol>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://jadenneal.github.io/2020/03/19/prioritized-experience-replay/" data-id="ck7yty1aw00009cpbe3vo6vth"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E4%BC%98%E5%85%88%E7%BB%8F%E9%AA%8C%E5%9B%9E%E6%94%BE/" rel="tag">优先经验回放</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/03/19/classical-DQN/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            经典DQN
          
        </div>
      </a>
    
    
      <a href="/2020/03/11/294-8/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">高级Q学习算法</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: '',
        app_key: '',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '想对作者说点什么',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2016-2020
        Jaden Neal
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Jaden Neal&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">目录</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E9%80%9A%E4%BF%A1%E7%A7%91%E6%99%AE">通信科普</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/latex">latex</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%85%B6%E4%BB%96">其他</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  
  </div>
</body>

</html>