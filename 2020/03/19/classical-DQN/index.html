<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    经典DQN |  Jaden Neal&#39;s Blog
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<meta name="generator" content="Hexo 4.2.0"></head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-classical-DQN" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  经典DQN
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/19/classical-DQN/" class="article-date">
  <time datetime="2020-03-19T14:52:00.000Z" itemprop="datePublished">2020-03-19</time>
</a>
      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>论文提出了深度学习模型成功应用于强化学习，可以说是将二者结合起来。模型为卷积神经网络，算法是<code>Q-learing</code>，输入为原始像素，输出是价值函数，可以估计未来的奖励。论文在7个Atari实验中进行测试，有6个游戏都超过了传统做法，其中有3个还超过了人类。</p>
<a id="more"></a>

<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>RL的一个长久的挑战就是直接从高维感官输入（比如视觉和语音）中学习控制智能体。很多成功的RL应用都是依赖于人工进行特征标注，因此这样的系统也严重依赖于特征表示的质量。</p>
<p>DL的进步使得从原始感官数据中提取高维特征称为可能，从而在CV和NLP领域取得重大突破。这些方法<strong>使用一系列的NN，包括CNN，多感知机，受限的玻尔兹曼机和RNN，并且利用了监督、无监督学习</strong>，论文受此启发，将这种技术应用在含有感官数据的RL工作中。</p>
<h3 id="RL的困难"><a href="#RL的困难" class="headerlink" title="RL的困难"></a>RL的困难</h3><p>一直以来，RL都对DL提出了几大挑战：</p>
<ol>
<li>DL模型都需要巨量人工标注的训练数据，<strong>而RL则相反</strong>，RL必须能够从频繁稀疏、有噪声和延迟的标量奖励中学习。与监督学习中的输入输出的直接联系相比，动作与后来的奖励之间的延迟将<strong>看起来非常尖锐</strong>。</li>
<li>大多数DL算法都假设数据是独立的，<strong>然而</strong>RL中的的状态序列都是高度相关的。</li>
<li>DL中的数据分布是不变的，<strong>然而</strong>RL中的数据分布会随着智能体学习到新的行为而改变。</li>
</ol>
<h3 id="论文的工作"><a href="#论文的工作" class="headerlink" title="论文的工作"></a>论文的工作</h3><p>论证了CNN在RL上的可行性，能够克服上述的一系列挑战。<br>大概做法：用<code>Q-learning</code>算法变量来训练网络，用<code>SGD</code>来更新参数，为了减少数据间的相关性和非稳态分布，还使用了<strong>经验回放</strong>。</p>
<p>最后，在Atari游戏上应用该模型，游戏环境的高维输入是<code>210 x 160 RGB, 60Hz刷新率</code>，模型只从视频输入中进行学习，7个游戏中由6个都超过了传统做法，效果很好。</p>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>考虑到任务中智能体与环境$\varepsilon$（即Atari模拟器）进行交互，得到一系列的状态，观察和奖励。动作输入到模拟器中，修改模拟器的内部状态和游戏分数。由于环境$\varepsilon$是随机的，因此智能体并不能观测到模拟器的内部状态，只能从模拟器中观察到图像$x_t \in \mathbb{R}^d$，这是原始像素值得一个向量，代表当前的图像。此外智能体接受的奖励$r_t$代表着游戏分数的改变。需要注意游戏分数大体上取决于整体的动作和观察的优先序列，关于动作的反馈也只是在数千个时间步之后。</p>
<p>由于智能体仅仅观察当前窗口的图像，因此该任务是部分可观察的，许多模拟器状态都会被误认为其他状态，比如他就不可能仅从当前窗口理解全部的状态$x_t$。因此论文考虑了一系列的动作和观察，$s_t = x_1, a_1, x_2, a_2, \ldots,a_{t-1}, x_t$，并基于这些序列学习游戏策略。模拟器中的所有序列都假定在有限的时间步内终止，这种形式产生了一个大而有限的马尔可夫决策过程（MDP），其中每个序列都是一个不同的状态。因此，仅仅通过使用$t$时刻的状态$s_t$，就可以将标准的RL应用于MDP上。</p>
<p>智能体的目标是通过选择动作与模拟器进行交互，来最大化未来的奖励。论文假设未来奖励在每一步有一个<strong>折扣因子</strong>$\gamma$，定义了未来的衰减回报为<br>$$R_t = \sum_{t^\prime = t}^T\gamma^{t\prime - t}r_{t\prime}$$这里$T$是时间步结束时间。<br>又定义了优化的动作值函数$Q^\star(s, a)$，表示遵循策略得到的最大期望回报。<br>$$Q^\star(s, a) = \max_\pi E[R_t|s_t = s, a_t = a, \pi]$$这里$\pi$是一个序列到动作的映射（也可以说是基于动作的分布）。</p>
<p>$Q^\star(s, a)$遵守一个重要定理——<strong>贝尔曼等式</strong>。这么说基于以下<strong>理由</strong>：倘若序列$s\prime$的优化值函数$Q^\star(s\prime, a\prime)$对于所有的动作$a\prime$都是已知的，那么优化策略就会选择$a\prime$来最大化$r + \gamma Q^\star(s\prime, a\prime)$的期望值，即<br>$$Q^\star(s, a) = E_{s\prime\sim\varepsilon}\left[r + \gamma\max_{a\prime}Q^\star(s\prime, a\prime)\vert s, a\right]$$ 许多RL算法背后的基本思想是<strong>使用贝尔曼等式作为迭代更新来预测动作-价值函数</strong>，即<br>$$Q_{i+1}(s, a) = E[r + \gamma\max_{a\prime}Q_i(s\prime, a\prime)\vert s, a]$$  这样的价值迭代算法会收敛于价值函数，用数学语言则表示为$Q_i \rightarrow Q^\star , i\rightarrow \infty$<br>但是，<strong>这种基本方法在实际中完全不实用</strong>，理由是由于每个序列的动作-价值函数都是分别预测的，没有任何的一般性归纳。然而，<strong>使用函数逼近器去预测动作-价值函数却很常见</strong>，即$Q(s, a;\theta) \approx Q^\star(s, a)$。在RL中这是一个典型的线性函数逼近器，当然有时也有非线性的，比如神经网络，人们通常将权重为$\theta$的神经网络函数逼近器称为Q网络，用每次迭代$i$后最小化损失函数$L_i(\theta_i)$来训练Q网络：<br>$$L_i(\theta_i) = E_{s, a\sim\rho(\cdot)}\left[(y_i - Q(s, a;\theta_i))^2\right]$$ 这里$y_i = E_{s\prime\sim\varepsilon}[r + \gamma\max_{a\prime}Q(s\prime, a\prime;\theta_{i-1})\vert s, a]$是每次迭代的目标，$\rho(s, a)$是一种基于序列$s$和动作$a$的概率分布，论文称之为<strong>动作分布</strong>。优化损失函数$L_i(\theta_i)$时，前一次迭代$\theta_{i-1}$的参数保持不变，值得注意的是基于网络权重的目标，与监督学习中的目标相反，后者在学习开始前是固定的。将损失函数与权值进行微分，得到如下梯度：<br>$$\nabla_{\theta_i}L_i(\theta_i) = E_{s, a\sim\rho(\cdot);s\prime}\sim\varepsilon\left[\left(r + \gamma\max_{a\prime}Q(s\prime, a\prime;\theta_{i-1}) - Q(s, a; \theta_i)\right)\nabla_{\theta_i}Q(s, a; \theta_i)\right]$$  相比于计算上述梯度中的全部期望值，论文采取了通过<code>SGD</code>来优化损失函数。如果每次时间步后权重都更新了，那么期望就会被来自于行为分布$\rho$和模拟器$\varepsilon$的单次样本值分别代替，于是就得到了类似于<code>Q-learning</code>的算法。</p>
<p>值得注意的是，这种算法有两个特点：</p>
<ol>
<li><strong>无模型</strong>的，他直接用来自模拟器$\varepsilon$的样本解决了RL任务，而不需要明确地构造$\varepsilon$的估计。 </li>
<li><strong>离线策略</strong>：该算法从贪心策略（$a = \max_aQ(s, a; \theta)$）学习，服从行为分布，又能确保对状态空间有足够的探索。在实际应用里，行为分布常常是由$\epsilon$贪心策略选择，服从参数为$1-\epsilon$的策略，并且以概率$\epsilon$选择动作。</li>
</ol>
<h2 id="之前的工作"><a href="#之前的工作" class="headerlink" title="之前的工作"></a>之前的工作</h2><p>TD-gammon，RL在博弈应用的经典，使用类似于<code>Q-learning</code>的五模型算法，然后用多层神经网络作为函数逼近。<strong>然而</strong>，后续对TD-gammon的深入则不那么成功，这一度让人们相信<code>TD-gammon</code>只是对<em>西洋双陆棋</em>有效果的一个特例。</p>
<p>而且，人们还发现，将无模型的RL算法例如<code>Q-learning</code>与非线性函数逼近器融合到一起（或者是离线策略学习），会导致Q网络发散。于是<strong>后续的RL的主要研究工作就是保证更好的收敛</strong>。</p>
<p>DNN用于估计环境$\varepsilon$，受限的玻尔兹曼机则用于估计价值函数，或者策略。此外，<strong>梯度时差方法部分地解决了发散问题</strong>。但是，这些方法还没能拓展到非线性控制部分。</p>
<p>之前的研究与DQN最相似的则是<code>NFQ</code>（神经拟合Q学习）。NFQ优化了损失函数序列，使用<code>RPROP</code>（弹性反向传播）算法更新Q网络的参数。<br>NFQ与DQN的对比</p>
<ol>
<li>首先是参数更新方式。<ul>
<li>NFQ使用的是一个批量更新，每次迭代的计算成本与数据集的大小成正比;</li>
<li>而DQN考虑的是随机梯度更新，每次迭代的恒定成本低，并且可以扩展到大型数据集。</li>
</ul>
</li>
<li>然后是学习方式。<ul>
<li>NFQ使用深度自编码器学习低维的任务表示；</li>
<li>DQN则直接从视觉输入，端到端地应用强化学习。因此可以学习与辨别动作值直接相关的特征。</li>
</ul>
</li>
</ol>
<h2 id="DQN概述"><a href="#DQN概述" class="headerlink" title="DQN概述"></a>DQN概述</h2><p>与<code>TD-gammon</code>和与之类似的在线算法不同，DQN利用的是<strong>经验回放</strong>。分为以下几步：</p>
<ol>
<li>在数据集D中存储这些经验，$e_t = (s_t, a_t, r_t, s_{t+1})$，把大量的回合（episode）池化进回放记忆。</li>
<li>在算法的内循环中，对从存储样本池中随机抽取的经验样本应用Q-学习更新，或小批量更新。</li>
<li>做完经验回放后，智能体根据$\epsilon-$贪心策略挑选一个动作并执行。</li>
<li>由于使用随机长度的记忆作为NN的输入很困难，因此DQN使用固定长度的记忆表示。</li>
</ol>
<h3 id="DQN算法"><a href="#DQN算法" class="headerlink" title="DQN算法"></a>DQN算法</h3><p>完整算法如下：</p>
<div align=center><img src="https://s1.ax1x.com/2020/03/22/85be6s.png" alt="85be6s.png" border="0" />
使用经验回放的深度Q学习</div>

<h3 id="DQN的优点"><a href="#DQN的优点" class="headerlink" title="DQN的优点"></a>DQN的优点</h3><ol>
<li>经验的每一步都有可能用于大量的权值更新，所以样本效率更好；</li>
<li>前面提到过多次，直接从连续样本值学习效率低下，这是由于样本间的极强的相关性，随机样本可以打破这种相关性，从而降低了更新的方差；</li>
<li>当学习策略时，当前参数将确定参数所使用的下一个数据样本。</li>
</ol>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><ol>
<li>正奖励均为1；负奖励均为-1；未变状态均为0</li>
<li>使用<code>RMSProp</code>算法，最小批次大小是32，训练策略为$\epsilon-$贪心策略，前100万帧参数$\epsilon$从1到0.1线性下降，随后固定在0.1。论文中一共训练了1000万帧，并使用了100万个最近帧的重放记忆。</li>
</ol>
<h3 id="稳定性"><a href="#稳定性" class="headerlink" title="稳定性"></a>稳定性</h3><p>虽然缺少理论上的收敛保证，但经过实践，DQN可以使用RL算法和SGD训练大的NN。</p>
<h3 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h3><p>见下图。</p>
<div align=center><img src="https://s1.ax1x.com/2020/03/22/8oPOjx.png" alt="8oPOjx.png" border="0"></div>

<p>上面五行比较的是DQN和其他算法的性能，均采用了$\epsilon-$贪心策略，参数$\epsilon = 0.05$<br>下面三行比较的是<code>HNeat</code>和DQN<strong>单回合更新</strong>的最佳表现结果，前者采用确定性策略，每次得到的分数相同；而DQN则采用$\epsilon-$贪心策略，$\epsilon = 0.05$。<br>可以发现，DQN的表现效果极佳。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://jadenneal.github.io/2020/03/19/classical-DQN/" data-id="ck835vju30000bspbf513ektw"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E7%BB%8F%E5%85%B8DQN/" rel="tag">经典DQN</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
    
      <a href="/2020/03/19/prioritized-experience-replay/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">优先经验回放</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: '',
        app_key: '',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '想对作者说点什么',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2016-2020
        Jaden Neal
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Jaden Neal&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">目录</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E9%80%9A%E4%BF%A1%E7%A7%91%E6%99%AE">通信科普</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/latex">latex</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%85%B6%E4%BB%96">其他</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  
  </div>
</body>

</html>