<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8" />
    
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
  <title>
    ACKTR |  Jaden Neal&#39;s Blog
  </title>
  
  <link rel="shortcut icon" href="/favicon.ico" />
  
  
<link rel="stylesheet" href="/css/style.css">

  
<script src="/js/pace.min.js"></script>


  

  

<meta name="generator" content="Hexo 4.2.0"></head>

</html>

<body>
  <div id="app">
    <main class="content">
      <section class="outer">
  <article id="post-ACKTR" class="article article-type-post" itemscope
  itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  ACKTR
</h1>
  

    </header>
    

    
    <div class="article-meta">
      <a href="/2020/03/27/ACKTR/" class="article-date">
  <time datetime="2020-03-27T14:11:16.000Z" itemprop="datePublished">2020-03-27</time>
</a>
      
    </div>
    

    
    
    <div class="tocbot"></div>





    

    <div class="article-entry" itemprop="articleBody">
      


      

      
      <p>本论文思维导图如下：</p>
<center><img src="https://s1.ax1x.com/2020/03/27/GiWIEV.png" alt="GiWIEV.png" border="0" /></center>

<a id="more"></a>

<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>论文的主要工作是<strong>拓展自然梯度，并使用带有信任域的K-FAC算法优化Actor和Critic</strong>，因此取名为<code>ACKTR</code>（音同Actor）。它有如下特点：</p>
<ol>
<li>AC（<code>Actor-Critic</code>）框架下，第一个可伸缩的信任域的，自然梯度方法。</li>
<li>直接从原始像素输入学习的方法，可在非平凡的任务中应用离散或连续控制。</li>
</ol>
<p>测试：</p>
<ul>
<li>离散任务：<code>Atari</code>游戏</li>
<li>连续任务：<code>MuJoCo</code>环境</li>
</ul>
<p>结果：对比当时最好的算法，样本效率提升了2-3倍，得到的奖励更高。</p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><h3 id="DRL和分布式方法"><a href="#DRL和分布式方法" class="headerlink" title="DRL和分布式方法"></a>DRL和分布式方法</h3><p><strong>DRL</strong>成果不少，但是采用的是<code>SGD</code>及其相关的<strong>一阶</strong>算法，这会导致搜索权重空间效率不高，花费时间巨大；后来有人提出了<strong>分布式方法</strong>，让多个agent同时与环境进行交互，减少了计算时间，但是样本效率又降低了。</p>
<h3 id="样本效率和自然梯度"><a href="#样本效率和自然梯度" class="headerlink" title="样本效率和自然梯度"></a>样本效率和自然梯度</h3><p><strong>样本效率</strong>是RL关心的主要问题之一。<strong>自然策略梯度使用自然梯度下降来更新梯度</strong>，从而可以有效减少样本大小。注意，自然梯度下降遵循<code>Fisher</code>指标，会沿着最快的下降方向。</p>
<h3 id="TRPO的不足"><a href="#TRPO的不足" class="headerlink" title="TRPO的不足"></a>TRPO的不足</h3><p><strong>自然梯度虽好，但是需要对Fisher矩阵求逆，所以精确计算是不可能的</strong>。<code>TRPO</code>则采用<code>Fisher</code>向量乘积的方法避免了直接求逆。但是，<code>TRPO</code>参数更新过于复杂，因此不实用，并且<code>TRPO</code>的样本效率也较低。</p>
<h3 id="K-FAC算法"><a href="#K-FAC算法" class="headerlink" title="K-FAC算法"></a>K-FAC算法</h3><p><code>K-FAC</code>，K系数曲率逼近，是一个可伸缩的对自然梯度的逼近。<code>K-FAC</code>每次更新的成本与<code>SGD</code>相当。<br><code>K-FAC</code>的特点：<strong>保持曲率信息的平均值，允许使用小批量</strong>。<br>该特点可以用来提升样本效率。</p>
<h3 id="ACKTR简介"><a href="#ACKTR简介" class="headerlink" title="ACKTR简介"></a>ACKTR简介</h3><p>特点：</p>
<ol>
<li>使用<code>K-FAC</code>逼近自然策略梯度，快速对协方差矩阵求逆</li>
<li>用高斯牛顿法优化值函数，从而拓展了自然梯度算法。</li>
<li>计算成本只比基于<code>SGD</code>的算法高10-25%，但是样本效率提升了2-3倍，奖励回报也大大提高。</li>
</ol>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="AC中的自然梯度"><a href="#AC中的自然梯度" class="headerlink" title="AC中的自然梯度"></a>AC中的自然梯度</h3><p>学习<code>Critic</code>可以看作是一个最小二乘函数逼近问题，选择的<strong>二阶</strong>算法试最为常用的高斯-牛顿法。<br>对于<code>Critic</code>，也就是<code>Value Function</code>，一般来说会输出一个值，但这里<strong>论文假设其输出的是一个高斯分布</strong>，并在实践中确定$\sigma = 1$效果最好。</p>
<h3 id="步长选择和信任域优化"><a href="#步长选择和信任域优化" class="headerlink" title="步长选择和信任域优化"></a>步长选择和信任域优化</h3><p>自然梯度的传统做法都是用<code>SGD</code>来更新，但这样会导致过大的更新，可能会导致算法过早地收敛到一个接近确定性的策略。<br>因此提出了<strong>信任域</strong>方法。根据这种方法，可以缩小更新范围，以最大某个规定的步长修改策略分布。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验目的"><a href="#实验目的" class="headerlink" title="实验目的"></a>实验目的</h3><p>做这些实验主要是为了研究以下问题：</p>
<ol>
<li>ACKTR与现有方法及常见二阶算法在<strong>样本效率和计算成本</strong>上的比较；</li>
<li>哪项参数可以让<code>Critic</code>的优化更好；</li>
<li>对比一阶算法，带有<code>batch size</code>伸缩的<code>ACKTR</code>的表现如何。</li>
</ol>
<h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><ol>
<li>离散控制：<code>Atari</code>游戏，平台是<code>OpenAI Gym</code>，由<code>Arcade Learning Environment</code>驱动；</li>
<li>连续控制：<code>MuJoCo</code>环境，平台还是<code>OpenAI Gym</code>。</li>
</ol>
<h3 id="baseline"><a href="#baseline" class="headerlink" title="baseline"></a>baseline</h3><ul>
<li>A3C</li>
<li>A2C</li>
<li>TRPO</li>
</ul>
<h3 id="实验步骤"><a href="#实验步骤" class="headerlink" title="实验步骤"></a>实验步骤</h3><ol>
<li><p>使用<code>AC</code>框架，</p>
<ul>
<li>对<code>Actor</code>：也就是<code>Policy function</code>，使用自然梯度下降；</li>
<li>对<code>Critic</code>：也就是<code>Value function</code>，使用高斯牛顿法。</li>
</ul>
</li>
<li><p>然后用<code>K-FAC</code>分别对A、C进行估计；</p>
</li>
<li><p><strong>离散部分</strong>，在<code>Atari</code>的6个游戏中，运行1千万个时间步（<strong>注：1个时间步为4帧</strong>），比较<code>ACKTR</code>、<code>A2C</code>和<code>TRPO</code>的奖励；</p>
</li>
<li><p>运行5千万个时间步，比较<code>ACKTR</code>、<code>A2C</code>和<code>TRPO</code>的最后100个回合的平均奖励；</p>
</li>
<li><p><strong>连续部分</strong>，在<code>MuJoCo</code>上的8个任务上，运行1百万个时间步，比较三者的最终奖励；</p>
</li>
<li><p>运行3千万个时间步，比较<code>ACKTR</code>、<code>A2C</code>和<code>TRPO</code>的最高的10个回合的平均奖励，以及达到规定标准（由一篇文献的结论）需要的回合数。</p>
</li>
</ol>
<h3 id="更多技术细节讨论"><a href="#更多技术细节讨论" class="headerlink" title="更多技术细节讨论"></a>更多技术细节讨论</h3><ol>
<li><code>K-FAC</code>采用的是一种分布式的实现——异步计算矩阵的逆。</li>
<li>使用信任域进行重（chong）伸缩更新，类似于<code>TRPO</code>的做法。</li>
<li>最为关键的一点，使用<strong>移动平均</strong>把Fisher矩阵的数据进行累计，从而可以得到更好的估计。</li>
</ol>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><h4 id="离散控制"><a href="#离散控制" class="headerlink" title="离散控制"></a>离散控制</h4><p>在<code>Atari</code>的6个游戏中，运行1千万个时间步（<strong>注：1个时间步为4帧</strong>），比较<code>ACKTR</code>、<code>A2C</code>和<code>TRPO</code>的奖励:</p>
<center><img src="https://s1.ax1x.com/2020/03/27/Gi1YAU.png" alt="图1" border="0"></center>

<p>可以看到在这6个游戏中，<code>ACKTR</code>的表现碾压其他两个。<br>接着训练5千万次，比较最后100个回合的平均奖励，以及要达到人类水平所需要的回合数。</p>
<center><img src="https://s1.ax1x.com/2020/03/27/Gi1lXq.png" alt="表1" border="0"></center>

<p>可见，在<code>Beamrider</code>、<code>Breakout</code>、<code>Pong</code>和<code>Q-bert</code>游戏中，<code>ACKTR</code>所花费的回合数都比<code>A2C</code>至少少了3倍。此外，在<code>Space Invaders</code>游戏中，<code>A2C</code>失败了，而<code>ACKTR</code>的分数大概是人类水平的12倍。不仅如此，<code>ACKTR</code>在各个游戏的分数也比<code>A2C</code>的高不少，<code>ACKTR</code>的优势可见一斑。</p>
<h4 id="连续控制"><a href="#连续控制" class="headerlink" title="连续控制"></a>连续控制</h4><p>在<code>MuJoCo</code>上的8个任务上，运行1百万个时间步，比较三者的最终奖励：</p>
<center><img src="https://s1.ax1x.com/2020/03/27/GiYCLR.png" alt="ACKTR3" border="0"></center>

<p><code>ACKTR</code>的优势很明显。<br>运行3千万个时间步，比较<code>ACKTR</code>、<code>A2C</code>和<code>TRPO</code>的最高的10个回合的平均奖励，以及达到规定标准（由一篇文献的结论）需要的回合数：</p>
<center><img src="https://s1.ax1x.com/2020/03/27/GiYpQJ.png" alt="ACKTR4" border="0"></center>

<h3 id="Critic优化的更好的范数"><a href="#Critic优化的更好的范数" class="headerlink" title="Critic优化的更好的范数"></a>Critic优化的更好的范数</h3><p>前人工作都是只优化<code>Actor</code>，<code>ACKTR</code>将<code>Critic</code>也纳入优化范围中。不同的是范数采用的是论文自己定义的：<br>范数$||\cdot||_B$来自于$||x||_B = (x^TBx)^{\frac{1}{2}}$，其中$B$是一个半正定矩阵。<br>结果发现，<strong>无论用哪种范数来优化Critic，ACKTR的表现总是或多或少地比A2C好，尤其是样本效率和回合分数方面</strong>。</p>
<h3 id="不同的batch-size"><a href="#不同的batch-size" class="headerlink" title="不同的batch size"></a>不同的batch size</h3><p>设置了两种<code>batch size</code>：160和640。</p>
<ul>
<li><code>ACKTR</code>：表现不变。</li>
<li><code>A2C</code>：<code>batch size</code>越大，效果越差。</li>
</ul>
<p>这说明在较大<code>batch size</code>下，使用<code>ACKTR</code>效果更好。</p>
<h2 id="个人思考"><a href="#个人思考" class="headerlink" title="个人思考"></a>个人思考</h2><p><code>ACKTR</code>的确是一个好算法，在各个方面的表现都超过了<code>A2C</code>和<code>TRPO</code>算法，尤其是样本效率以及回合奖励方面。</p>
<p>本篇论文值得借鉴的地方有两点：</p>
<ol>
<li>创新点：融合各种算法，<code>ACKTR</code>的实质就是将<code>K-FAC</code>应用于AC框架，仅仅是实践上的创新，使得模型有了更好的表现。</li>
<li>方法论：主要就是对比试验，与各种算法进行比较，固定训练时间比结果，固定结果比耗费时间，等等，这就从许多角度论证了该算法的优越性。</li>
</ol>
<p>但是，不难发现，<code>ACKTR</code>也有缺点，<strong>就是样本效率的问题</strong>。在最后的讨论中，作者用了160和640两种<code>batch size</code>进行试验，结果就是<code>ACKTR</code>在这两种情况下的表现并无差别，而<code>A2C</code>的表现却随着<code>batch size</code>的提高而下降了，这是在比烂，因为<code>ACKTR</code>的表现并没有更好。当然这可能属于<code>batch size</code>对模型的影响，表现不变差就可以称得上“较好”了。<br>最后，作者也给出优化思路，就是针对样本效率问题进行改进，参见<code>BDPI</code>算法；未来的工作方向则是考虑将不同的算法融合起来，应用起来，就有可能得到表现更好的模型。</p>

      
      <!-- 打赏 -->
      
    </div>
    <footer class="article-footer">
      <a data-url="https://jadenneal.github.io/2020/03/27/ACKTR/" data-id="ck8fvo0sk000go4pbfre37jsm"
        class="article-share-link">分享</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ACKTR/" rel="tag">ACKTR</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/" rel="tag">论文阅读</a></li></ul>

    </footer>

  </div>

  
  
  <nav class="article-nav">
    
      <a href="/2020/04/04/%E5%8D%B7%E7%A7%AF%E7%A0%81%E5%92%8C%E7%BB%B4%E7%89%B9%E6%AF%94%E8%AF%91%E7%A0%81/" class="article-nav-link">
        <strong class="article-nav-caption">上一篇</strong>
        <div class="article-nav-title">
          
            卷积码和维特比译码
          
        </div>
      </a>
    
    
      <a href="/2020/03/26/A-survey-of-underwater-wireless-optical-communication/" class="article-nav-link">
        <strong class="article-nav-caption">下一篇</strong>
        <div class="article-nav-title">水下光通信研究</div>
      </a>
    
  </nav>


  

  
  
<!-- valine评论 -->
<div id="vcomments-box">
    <div id="vcomments">
    </div>
</div>
<script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src='https://cdn.jsdelivr.net/npm/valine@1.3.10/dist/Valine.min.js'></script>
<script>
    new Valine({
        el: '#vcomments',
        notify: false,
        verify: false,
        app_id: '',
        app_key: '',
        path: window.location.pathname,
        avatar: 'mp',
        placeholder: '想对作者说点什么',
        recordIP: true
    });
    const infoEle = document.querySelector('#vcomments .info');
    if (infoEle && infoEle.childNodes && infoEle.childNodes.length > 0) {
        infoEle.childNodes.forEach(function (item) {
            item.parentNode.removeChild(item);
        });
    }
</script>
<style>
    #vcomments-box {
        padding: 5px 30px;
    }

    @media screen and (max-width: 800px) {
        #vcomments-box {
            padding: 5px 0px;
        }
    }

    #vcomments-box #vcomments {
        background-color: #fff;
    }

    .v .vlist .vcard .vh {
        padding-right: 20px;
    }

    .v .vlist .vcard {
        padding-left: 10px;
    }
</style>

  

  
  
  

</article>
</section>
      <footer class="footer">
  <div class="outer">
    <ul class="list-inline">
      <li>
        &copy;
        2016-2020
        Jaden Neal
      </li>
      <li>
        
      </li>
    </ul>
    <ul class="list-inline">
      <li>
        
        
        <ul class="list-inline">
  <li>PV:<span id="busuanzi_value_page_pv"></span></li>
  <li>UV:<span id="busuanzi_value_site_uv"></span></li>
</ul>
        
      </li>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
    <div class="to_top">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>
      </div>
    </main>
    
    <aside class="sidebar">
      
        <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Jaden Neal&#39;s Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">目录</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0">深度学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0">强化学习</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E7%88%AC%E8%99%AB">爬虫</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E9%80%9A%E4%BF%A1%E7%A7%91%E6%99%AE">通信科普</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/latex">latex</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags/%E5%85%B6%E4%BB%96">其他</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
      </aside>
      <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
      
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/jquery.justifiedGallery.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>




  
<script src="/js/tocbot.min.js"></script>

  <script>
    // Tocbot_v4.7.0  http://tscanlin.github.io/tocbot/
    tocbot.init({
      tocSelector: '.tocbot',
      contentSelector: '.article-entry',
      headingSelector: 'h1, h2, h3, h4, h5, h6',
      hasInnerContainers: true,
      scrollSmooth: true,
      positionFixedSelector: '.tocbot',
      positionFixedClass: 'is-position-fixed',
      fixedSidebarOffset: 'auto',
    });
  </script>


<script>
  var ayerConfig = {
    mathjax: true
  }
</script>


<script src="/js/ayer.js"></script>


<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">


  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.6/unpacked/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<script type="text/javascript" src="https://js.users.51.la/20544303.js"></script>
  
  
  </div>
</body>

</html>